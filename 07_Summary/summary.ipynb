{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Compare model results and final model selection\n",
    "\n",
    "Using the Titanic dataset from [this](https://www.kaggle.com/c/titanic/overview) Kaggle competition.\n",
    "\n",
    "In this section, we will do the following:\n",
    "1. Evaluate all of our saved models on the validation set\n",
    "2. Select the best model based on performance on the validation set\n",
    "3. Evaluate that model on the holdout test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from time import time\n",
    "\n",
    "val_features = pd.read_csv('C:/Users/VK/MLPython/Ex_Files_Machine_Learning_Algorithms/val_features.csv')\n",
    "val_labels = pd.read_csv('C:/Users/VK/MLPython/Ex_Files_Machine_Learning_Algorithms/val_labels.csv')\n",
    "\n",
    "te_features = pd.read_csv('C:/Users/VK/MLPython/Ex_Files_Machine_Learning_Algorithms/test_features.csv')\n",
    "te_labels = pd.read_csv('C:/Users/VK/MLPython/Ex_Files_Machine_Learning_Algorithms/test_labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "\n",
    "for mdl in ['LR', 'SVM', 'MLP', 'RF', 'GB']:\n",
    "    models[mdl] = joblib.load('C:/Users/VK/MLPython/Ex_Files_Machine_Learning_Algorithms/{}_model.pkl'.format(mdl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate models on the validation set\n",
    "\n",
    "![Evaluation Metrics](../../img/eval_metrics.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(name, model, features, labels):\n",
    "    start = time()\n",
    "    pred = model.predict(features)\n",
    "    end = time()\n",
    "    accuracy = round(accuracy_score(labels, pred), 3)\n",
    "    precision = round(precision_score(labels, pred), 3)\n",
    "    recall = round(recall_score(labels, pred), 3)\n",
    "    print('{} -- Accuracy: {} / Precision: {} / Recall: {} / Latency: {}ms'.format(name,\n",
    "                                                                                   accuracy,\n",
    "                                                                                   precision,\n",
    "                                                                                   recall,\n",
    "                                                                                   round((end - start)*1000, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR -- Accuracy: 0.827 / Precision: 0.846 / Recall: 0.724 / Latency: 4.0ms\n",
      "SVM -- Accuracy: 0.799 / Precision: 0.794 / Recall: 0.711 / Latency: 4.0ms\n",
      "MLP -- Accuracy: 0.81 / Precision: 0.828 / Recall: 0.697 / Latency: 4.0ms\n",
      "RF -- Accuracy: 0.816 / Precision: 0.852 / Recall: 0.684 / Latency: 16.0ms\n",
      "GB -- Accuracy: 0.816 / Precision: 0.852 / Recall: 0.684 / Latency: 4.0ms\n"
     ]
    }
   ],
   "source": [
    "for name, mdl in models.items():\n",
    "    evaluate_model(name,mdl, val_features, val_labels )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate best model on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest -- Accuracy: 0.803 / Precision: 0.778 / Recall: 0.646 / Latency: 16.0ms\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#evaluate test and validate results are close\n",
    "evaluate_model('Random Forest', models['RF'],te_features, te_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression -- Accuracy: 0.775 / Precision: 0.712 / Recall: 0.646 / Latency: 0.0ms\n"
     ]
    }
   ],
   "source": [
    "evaluate_model('Logistic Regression', models['LR'],te_features, te_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting -- Accuracy: 0.815 / Precision: 0.808 / Recall: 0.646 / Latency: 8.0ms\n"
     ]
    }
   ],
   "source": [
    "evaluate_model('Gradient Boosting', models['GB'],te_features, te_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
